{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product Categories Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZo7VkO-0Bo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import nltk.corpus\n",
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "file_embedding = \"idwiki_word2vec_100.model\"\n",
        "'''id_w2v = gensim.models.word2vec.Word2Vec.load(file_embedding)\n",
        "print(id_w2v.most_similar('sepatu'))'''\n",
        "category_index = {\"olahraga\":1, \"pertukangan\":2, \"fashion\":3, \"elektronik\":4, \"handphone\":5}\n",
        "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
        "STOPWORDS = set(stopwords.words(\"indonesian\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zsWrAytLrK9"
      },
      "source": [
        "**LOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7Dvncp_LOlk",
        "outputId": "f38efe84-41d6-4d2f-b8e9-badcf709b148"
      },
      "source": [
        "olahraga = pd.read_csv(\"olahraga.csv\", sep=',')\n",
        "pertukangan = pd.read_csv(\"pertukangan.csv\", sep=',')\n",
        "fashion = pd.read_csv(\"fashion.csv\", sep=',')\n",
        "elektronik = pd.read_csv(\"elektronik.csv\", sep=',')\n",
        "handphone = pd.read_csv(\"handphone.csv\", sep=',')\n",
        "\n",
        "datasets = [olahraga, pertukangan, fashion, elektronik, handphone]\n",
        "\n",
        "print(\"Make sure there are no null values in the datasets\")\n",
        "for data in datasets:\n",
        "    print(\"Has null values: \", data.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make sure there are no null values in the datasets\n",
            "Has null values:  False\n",
            "Has null values:  False\n",
            "Has null values:  False\n",
            "Has null values:  False\n",
            "Has null values:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNdwS1f7Luif"
      },
      "source": [
        "**PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzJISGv1Lo-w"
      },
      "source": [
        "def preprocess(text):\n",
        "    text= text.strip().lower().split()\n",
        "    text = filter(lambda word: word not in STOPWORDS, text)\n",
        "    return \" \".join(text)\n",
        "    \n",
        "for dataset in datasets:\n",
        "    dataset['title'] = dataset['title'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDHQP9MPwQZ"
      },
      "source": [
        "all_texts = olahraga['title'] + pertukangan['title'] + fashion['title'] + elektronik['title'] + handphone['title']\n",
        "all_texts = all_texts.drop_duplicates(keep=False)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "\n",
        "olahraga_sequences = tokenizer.texts_to_sequences(olahraga['title'])\n",
        "pertukangan_sequences = tokenizer.texts_to_sequences(pertukangan['title'])\n",
        "fashion_sequences = tokenizer.texts_to_sequences(fashion['title'])\n",
        "elektronik_sequences = tokenizer.texts_to_sequences(elektronik['title'])\n",
        "handphone_sequences = tokenizer.texts_to_sequences(handphone['title'])\n",
        "\n",
        "olahraga_data = pad_sequences(olahraga_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "pertukangan_data = pad_sequences(pertukangan_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "fashion_data = pad_sequences(fashion_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "elektronik_data = pad_sequences(elektronik_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "handphone_data = pad_sequences(handphone_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxuqxfJzQoPt",
        "outputId": "c9df7e87-9dd5-48f8-d19e-30bf0394eeec"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "test_string = \"sepatu nike terbaru\"\n",
        "print(\"word\\t\\tid\")\n",
        "print(\"-\" * 20)\n",
        "for word in test_string.split():\n",
        "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word\t\tid\n",
            "--------------------\n",
            "sepatu\t\t10\n",
            "nike\t\t7\n",
            "terbaru\t\t290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AV7PfSpQ9LZ",
        "outputId": "d396989c-c9d4-4613-b014-9b8e2a5fa721"
      },
      "source": [
        "test_sequence = tokenizer.texts_to_sequences([\"sepatu nike terbaru\", \"sepatu adidas terbaru\"])\n",
        "padded_sequence = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"Text to Vector\", test_sequence)\n",
        "print(\"Padded Vector\", padded_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to Vector [[10, 7, 290], [10, 113, 290]]\n",
            "Padded Vector [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0  10   7 290]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0  10 113 290]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jit0SNCRXQI",
        "outputId": "b4b14577-c094-4050-ed3b-44b9effc1abe"
      },
      "source": [
        "print(\"olahraga: \\t\", to_categorical(category_index[\"olahraga\"], 6))\n",
        "print(\"pertukangan: \\t\", to_categorical(category_index[\"pertukangan\"], 6))\n",
        "print(\"fashion: \\t\", to_categorical(category_index[\"fashion\"], 6))\n",
        "print(\"elektronik: \\t\", to_categorical(category_index[\"elektronik\"], 6))\n",
        "print(\"handphone: \\t\", to_categorical(category_index[\"handphone\"], 6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "olahraga: \t [0. 1. 0. 0. 0. 0.]\n",
            "pertukangan: \t [0. 0. 1. 0. 0. 0.]\n",
            "fashion: \t [0. 0. 0. 1. 0. 0.]\n",
            "elektronik: \t [0. 0. 0. 0. 1. 0.]\n",
            "handphone: \t [0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-j1EvcWRzl-",
        "outputId": "4c4e2375-0464-4cbd-986c-ce1531c1aa09"
      },
      "source": [
        "print(\"olahraga shape: \", olahraga_data.shape)\n",
        "print(\"pertukangan shape: \", pertukangan_data.shape)\n",
        "print(\"fashion shape: \", fashion_data.shape)\n",
        "print(\"elektronik shape: \", elektronik_data.shape)\n",
        "print(\"handphone shape: \", handphone_data.shape)\n",
        "\n",
        "data = np.vstack((olahraga_data, pertukangan_data, fashion_data, elektronik_data, handphone_data))\n",
        "category = pd.concat([olahraga['category'], pertukangan['category'], fashion['category'], elektronik['category'], handphone['category']]).values\n",
        "category = to_categorical(category)\n",
        "print(\"-\"*10)\n",
        "print(\"combined data shape: \", data.shape)\n",
        "print(category)\n",
        "print(\"combined category/label shape: \", category.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "olahraga shape:  (7838, 30)\n",
            "pertukangan shape:  (1826, 30)\n",
            "fashion shape:  (8910, 30)\n",
            "elektronik shape:  (15897, 30)\n",
            "handphone shape:  (6136, 30)\n",
            "----------\n",
            "combined data shape:  (40607, 30)\n",
            "[[0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            "combined category/label shape:  (40607, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT1nd6BNSvES"
      },
      "source": [
        "VALIDATION_SPLIT = 0.4\n",
        "indices = np.arange(data.shape[0]) # get sequence of row index\n",
        "np.random.shuffle(indices) # shuffle the row indexes\n",
        "data = data[indices] # shuffle data/product-titles/x-axis\n",
        "category = category[indices] # shuffle labels/category/y-axis\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = category[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = category[-nb_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXr1Aa7dTGP9"
      },
      "source": [
        "**WORD2VEC EMBEDDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbHLsPsqTLeu",
        "outputId": "ffe52086-4c15-4068-ae76-74dffb858cec"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load(file_embedding)\n",
        "print('Found %s word vectors of word2vec' % len(word2vec.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 331792 word vectors of word2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlEWxw_YX9VR",
        "outputId": "30773cd3-d5bb-4448-c40a-e645cbeb66f9"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec.wv.vocab:\n",
        "        embedding_matrix[i] = word2vec.wv.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
        "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgTLvL8pZOr0"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEzmuahgZVcx",
        "outputId": "22546aff-dce9-4163-af4a-3d52afefdad1"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(embedding_layer)\n",
        "model_1.add(Conv1D(50,5,activation='relu'))\n",
        "model_1.add(GlobalMaxPooling1D())\n",
        "model_1.add(Dense(50))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dense(6))\n",
        "model_1.add(Activation('softmax'))\n",
        "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 30, 100)           80100     \n",
            "_________________________________________________________________\n",
            "conv1d_79 (Conv1D)           (None, 26, 50)            25050     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_22 (Glo (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 6)                 306       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 108,006\n",
            "Trainable params: 27,906\n",
            "Non-trainable params: 80,100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft43GxlycG2K",
        "outputId": "36f0977f-659b-471a-c114-71750c862344"
      },
      "source": [
        "model_1.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
        "score = model_1.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "191/191 [==============================] - 6s 24ms/step - loss: 0.5194 - acc: 0.8291 - val_loss: 0.1369 - val_acc: 0.9515\n",
            "Epoch 2/5\n",
            "191/191 [==============================] - 4s 22ms/step - loss: 0.1379 - acc: 0.9493 - val_loss: 0.1175 - val_acc: 0.9534\n",
            "Epoch 3/5\n",
            "191/191 [==============================] - 4s 22ms/step - loss: 0.1104 - acc: 0.9547 - val_loss: 0.1019 - val_acc: 0.9600\n",
            "Epoch 4/5\n",
            "191/191 [==============================] - 4s 22ms/step - loss: 0.0950 - acc: 0.9602 - val_loss: 0.1025 - val_acc: 0.9595\n",
            "Epoch 5/5\n",
            "191/191 [==============================] - 4s 22ms/step - loss: 0.0929 - acc: 0.9610 - val_loss: 0.1031 - val_acc: 0.9605\n",
            "Test loss: 0.10311932116746902\n",
            "Test accuracy: 0.960472822189331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZhyPZxSh8-t",
        "outputId": "68cfe1c6-6451-410f-e93b-6c3636429f09"
      },
      "source": [
        "example_product = \"Paket Headband Lace Tamagoo Bandana Rambut Anak baby Premium\"\n",
        "example_product = preprocess(example_product)\n",
        "example_sequence = tokenizer.texts_to_sequences([example_product])\n",
        "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"-\"*10)\n",
        "#print(\"Predicted category: \", category_reverse_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
        "print(\"Predicted category: \", category_reverse_index[np.argmax(model_1.predict(example_padded_sequence), axis=-1)[0]])\n",
        "print(\"-\"*10)\n",
        "probabilities = model_1.predict(example_padded_sequence, verbose=0)\n",
        "probabilities = probabilities[0]\n",
        "print(\"Olahraga Probability: \",probabilities[category_index[\"olahraga\"]] )\n",
        "print(\"Pertukangan Probability: \",probabilities[category_index[\"pertukangan\"]] )\n",
        "print(\"Fashion probability: \",probabilities[category_index[\"fashion\"]] )\n",
        "print(\"Elektronik probability: \",probabilities[category_index[\"elektronik\"]] )\n",
        "print(\"Handphone probability: \",probabilities[category_index[\"handphone\"]] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "Predicted category:  fashion\n",
            "----------\n",
            "Olahraga Probability:  0.00019297855\n",
            "Pertukangan Probability:  2.6492437e-09\n",
            "Fashion probability:  0.9992138\n",
            "Elektronik probability:  0.00023739394\n",
            "Handphone probability:  0.00035584954\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}